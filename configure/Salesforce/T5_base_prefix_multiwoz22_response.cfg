[model]
name = unified.prefixtuning
use_description = False
;True
concatenate_description = True
# Should be one of (separate, concatenate)
;knowledge_usage = None
;knowledge_usage to 'concatenate' or choose to not set it, it will use the basic version of Prefix-tuning,
;but if you use set it to the 'separate', it will be our changes on the prefix passing mechanism(but we found not work.).
; use context as input to prompt encoder
use_state = True
freeze_plm = True
freeze_prefix = False

[dataset]
data_store_path = ./data
upsample_temp = 1

[seq2seq]
constructor = seq2seq_construction.meta_tuning
patience = 200
debug = False

[arg_paths]
# Conversational
multiwoz = META_TUNING/multiwoz22_response.cfg

[evaluate]
tool = metrics.meta_tuning.evaluator

[prefix_tuning]
prefix_sequence_length = 10
mid_dim = 512
prefix_dropout = 0.0

[special_tokens]
less = ' <'
less_or_equal = ' <='

[bert]
location = t5-base