[model]
name = unified.prefixtuning
use_description = False
concatenate_description = True
# Should be one of (separate, concatenate)
knowledge_usage = concatenate
freeze_plm = True
freeze_prefix = False
##TODO add an arg not using knowledge set struct_in to None

[dataset]
data_store_path = ./data
upsample_temp = 1

[seq2seq]
constructor = seq2seq_construction.meta_tuning
patience = 200

[arg_paths]
# Conversational
multiwoz = META_TUNING/multiwoz_dialog.cfg

[evaluate]
tool = metrics.meta_tuning.evaluator

[prefix_tuning]
prefix_sequence_length = 10
mid_dim = 512
prefix_dropout = 0.0

[special_tokens]
less = ' <'
less_or_equal = ' <='

[bert]
location = t5-base